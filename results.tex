%!TEX root = paper.tex

\section{BRATS challenge: framework details}

\subsection{Training dataset (BRATS 2015)}

The BRATS 2015 dataset is made available to challenge participants for training. It contains $274$ images along with their ground truth annotations. One of the interesting aspects of the dataset is the nature of ground truth annotations: $30$ images (from the BRATS 2013 dataset) were manually annotated and the remaining are annotated using a consensus of segmentation algorithms. While the ground truth is often of good quality, we note with interest that the consensus of algorithms generally fails at correctly labelling post-resection cavities as in Fig. \ref{fig: post-resection cavities} (bottom row). This is no doubt due to the fact that there is only \textit{one} such training example in the original BRATS 2013 dataset (Fig. \ref{fig: post-resection cavities}, top row). 

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{images/BRATS2015_post-resection-cavities.png}
\label{fig: post-resection cavities}
\caption{Auto-Context Segmentation Forests. In this schematic example, layer $2$ solves a segmentation task distinct from that of layers $1$, $3$ but the interleaving allows to exploit joint dependencies.}
\end{figure}

We paid particular attention to such training examples. For these cases we favoured a qualitative, visual assessment of correctness over quantitative metrics (DICE overlap or Hausdorff distance) when tuning our pipeline. These cases and similar observations motivate the two following choices: 1) An unsupervised SMM/MRF (see below) is trained on the $30$ manually annotated BRATS 2013 images, to initialize the segmentation pipeline. For the background class, SMM weights are spatially varying, so that the model proves reasonably effective to disambiguate potential post-resection cavities from, say, ventricles; and 2) $70$ images from the BRATS 2015 dataset ground truth with high quality annotations are chosen and used for training of the final model. While leading to a slight decrease in quantitative performance of the algorithm, it also qualitatively somewhat improves segmentation results (Fig. \ref{fig: post-resection cavities}, last column). The same qualitative observations are made on the BRATS 2016 test set. 

\subsection{Pipeline, model and parameter settings}

\noindent
\textbf{Preprocessing.} Image masks are defined from the FLAIR modality, masking out voxels of intensity $0$. The image contrast is standardized: the distribution of voxel intensities within the mask is brought to a preset, common median and mean absolute error by affine remapping. As a result images are all normalized within the same intensity range, so that the following step is mostly implementation specific. We further window intensity values to make threshold quantization easier when training DFs: intensities are thresholded to lie between some minimum and maximum values and brought within a byte range.\\

\noindent
\textbf{Initialization: SMM/MRF.} An SMM-MRF layer is used to locate the region of interest (ROI) for the whole tumor. The likelihood for each of the five mutually exclusive ground truth classes is modelled using a Student Mixture (SMM) with spatially varying (BG) or fixed (other classes) proportions~\cite{archambeau2007robust}, as a suitably modified variant of~\cite{cordier2015patch}. An MRF prior is assumed over BG, ED and TC. The model is similar in spirit to~\cite{zhang2001segmentation,menze2010generative}. The model is purely unsupervised: we do not use white/grey matter and cerebro-spinal fluid labels in the current implementation. However the learnt components for the background SMM are highly correlated to those labels. We assume another MRF prior over voxel assignments to the background SMM components, encoding the rough intuition that WM/GM/CSF should smoothly vary spatially. Variational Bayesian inference is used at training and test time. Both MRFs define fully connected cliques over the image, with Gaussian decay of pairwise potentials w.r.t. the distance of voxel centers. For this choice of potentials, the dependencies induced by MRF priors in variational updates can be efficiently computed via Gaussian filtering. Inference over $3D$ volumes is very fast both at training (seconds or minutes) and test time (seconds).\\

\noindent
\textbf{Auto-context architecture.} $9$ layers of binary DFs are cascaded, cycling between WT, TC and ET probabilities. All layers use the original, raw image channels. The first layers have their input augmented with probability maps from the upstream SMM/MRF, the subsequent layers use probability maps output by previous DFs. In addition, the first $3$ layers are also passed the prior probability "atlas" maps returned by the spatially-varying background SMM/MRF model. Many variants of this architecture were informally tested without demonstrating a significant effect on accuracy.\\

\noindent
\textbf{ROI refinement.} For computational convenience, subsequent layers run on ROIs rather than the full image. For instance, the second WT binary DF only tests points within the mask provided by the first WT binary DF. Similarly at training time, the second layer is trained on subsets of image voxels within the respective image ROIs output by the first layer. The first, second and third layers use masks obtained as dilated versions of the segmentation masks output by the previous layer (dilated resp. by $15$mm, $10$mm, $5$mm).\\

\noindent
\textbf{Parameter settings.} DFs come with a number of parameters, most of which where not found to drastically affect the pipeline accuracy following many informal experiments. Five feature types are used: intensity in a given channel (respectively, at scale $s$), difference of intensities between the voxel of interest (VOI) at scale $s_1$ and offset voxel at scale $s_2 > s_1$ in a given channel, median of the intensity difference (respectively, absolute difference) between the VOI at scale $s_1$ and the radius-$r$ icosahedron vertices (scale $s_2>s_1$) in a given channel. Between $100$ and $200$ candidate features are sampled per node. We use $2$ scales: $1$mm and $2$mm for the first layers, $0.5$mm and $1$mm for the second and final layers. The voxel offset along each direction / icosahedron radius is sampled uniformly between $0$ and $50$mm. The range of feature responses is quantized using $50$ thresholds. The maximum tree depth is set at $12$, and is seldom reached. The number of decision pathways clusters (section \ref{sec: clustering}) is set to $4$. They are created using the first layer of WT, TC, ET (separately for each classification task). The subsampling rate for data bagging is adjusted based on the desired computation time. At each node, the training voxels from $25$ random images serve as tuning set and similarly $30$ (distinct) random images are used for validation (the remaining images are not used to train the node).

\subsection{Test dataset (BRATS 2016)}

The pipeline described above is fully automated. To our knowledge, the BRATS 2015 training dataset pre-processing includes rigid registration (as well as resampling to a common image geometry), bias field correction and skull removal~\cite{menze2015multimodal}. The BRATS 2016 test dataset contains a number of unprocessed or partially pre-processed images (cf.~Fig.~\ref{fig: test data}). To cope with that, the pipeline was modified to include rigid registration and resampling, bias field correction~\cite{tustison2009n4itk} and skull removal as part of a semi-automatic pre-processing step.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{images/BRATS2016_example-data.png}
\label{fig: test data}
\caption{BRATS 2016 test data: example of variability not seen in the training set. (Left) Bias field (Middle) Partial skull stripping (Right) Rigid misalignment, different geometry.}
\end{figure}

\subsection{Experimental setting \& running time}

The proposed approach is implemented in C\# and F\#. All experiments were performed on a 3.6GHz Intel Xeon processor system with 16GB RAM running Microsoft Windows 10. Training on the BRATS 2015 dataset takes $6$ to $7$ hours (including "testing" on the whole dataset). Testing takes about $20$s per image.

\section{Experiments \& Results}
\label{sec: results}



%\subsection{Hold-out estimates, auto-context, guided bagging: an analysis}
%This is what we have been doing throughout the paper. Singling out individual contributions.

\subsection{BRATS benchmark: Multi-modal MR brain tumor segmentation}

Report running times! Compare to the literature! Try to do the BRATS 2013 leaderboard comparison (array of numbers). BRATS 2015 accuracy vs. number of training images, with varying train/test subsets. Compare to AutoGlioS baseline + to 1-layer CVE forests. Report predicted accuracy (training). Compare to some numbers reported in the literature, point out that our test accuracy was closer.

\subsection{Multi-organ segmentation from CT scans}
