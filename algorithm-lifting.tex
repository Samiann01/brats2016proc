%!TEX root = paper.tex

\section{Lifting Decision Forests by Minimizing Cross-Validation Error Estimates}
\label{sec: lifting}

\todo{cautionary?}
\subsection{A cautionary look at Information Gain maximization}

We follow the notation introduced in Section~\ref{sec: background}, but drop the node index $n$ for convenience. 
Information gain maximization w.r.t. (feature, threshold) parameters $\psi=(\bm{\theta},\tau)$ can be shown to be equivalent to a joint maximum likelihood estimation (MLE) of $\phi\!\triangleq\!(\psi,p_L,p_R)$, the node parameters and children' class predictors. We omit the details for the sake of brevity. Essentially, decision trees are usually grown by 
greedily, recursively splitting leaf nodes by likelihood maximization:
\begin{equation}
\phi^\ast = \argmax_{\phi} \; \calC\!\calF(D;\phi) \triangleq \frac{p(D;\phi)}{p^\ast(D)} \, .
\label{eq: ML maximization}
\end{equation}
In Eq.~\eqref{eq: ML maximization}, the denominator is the data likelihood using the current leaf node predictor, whereas the numerator is the data likelihood when splitting this node with parameters $\phi$ into left and right children. The denominator is constant w.r.t. $\phi$, as optimization of the current node has precedence in the recursive schedule. 

Unfortunately, with MLE, we run the risk of overfitting. With DFs the risk is twofold. At a node-level, weak learners with poor generalization may be selected. The deeper the trees, the more likely it is to happen, since the training data is split between an exponentially increasing number of nodes. At a tree-level, the lack of principled method
for controlling model capacity negatively impacts generalization. Indeed, the information gain of Eq.~\eqref{eq: ML maximization} is strictly positive as long as 1) training samples remain at the node of interest, and 
  2) the data distribution is not pure. As a result, trees generally grow to the maximum allowed depth, with little control over generalization. Medical image segmentation tasks often require large trees of weak learners to be grown (tree depth $20$--$30$, millions of nodes). Due to computational constraints, few such trees can be grown (a few dozens at most), so that model averaging across randomized trees is insufficient to balance tree overfitting. As an efficient alternative to MLE that can be directly used to control tree growth and generalization, we propose to maximize the predictive score as obtained from \textit{cross-validation} estimates.

\subsection{Maximizing Cross Validation Estimates of Generalization}   
\label{sec: CVE}

We derive Cross-Validation Estimates (CVE) of the predictive score as follows. At any given node, the (potentially bagged) training data $D=D^{\text{CV}}\!\coprod\! D^{\text{T}}$ is randomly divided in two disjoint subsets, a tuning subset $D^{\text{T}}$ and a validation subset $D^{\text{CV}}$. The optimization problem is then defined as:
\begin{equation}
\begin{split}
\phi^\ast = & \,\argmax_{\phi} \cfrac{p(D^{\text{CV}};\bm{\theta})}{\raisebox{-0.5ex}{$p^\ast(D^{\text{CV}})$}} \; , \quad \text{s.t. }\\[\medskipamount]
%& p^\ast\left(D^{\text{CV}};\bm{\theta}\right) \triangleq p\left(D^{\text{CV}};\bm{\theta}, \tau(\bm{\theta}), p_\epsilon(\cdot;\bm{\theta})\right) \\[\medskipamount]
& \big(\tau_{\bm{\theta}}, p_\epsilon(\cdot;\bm{\theta})\big) = \,\argmax_{\tau, p_\epsilon} p_\epsilon(D_\epsilon^{\text{T}};\phi)
\end{split}
\label{eq: CV optimization}
\end{equation}
where $p(D^{\text{CV}};\bm{\theta})\!\triangleq\! p(D^{\text{CV}};\bm{\theta}, \tau_{\bm{\theta}}, p_\epsilon(\cdot;\bm{\theta}))$. The key change is that parameters are now constrained to be tuned on $D^{\text{T}}$ whereas the final feature score is computed on $D^{\text{CV}}$. While a \textit{k-fold} estimate could be used instead in Eq.~\eqref{eq: CV optimization}, the \textit{hold-out} procedure has the benefit of efficiency and added randomness. 

A simple two stage procedure is followed, akin to that of MLE.% (Algo \ref{}). 
For each candidate feature $\bm{\theta}$, the best threshold $\tau_{\bm{\theta}}$ is found by IG maximization on the tuning subset $D^{\text{T}}$. Left and right children' class predictors are set to the empirical tuning data histograms. Instead of directly returning the IG as a feature score, the cross-entropy between tuning and validation empirical distributions is computed and returned. As we merely replace the ML feature score by a CV score computed from subsets of readily available data, the computational complexity remains unchanged.

Key to the proposed approach is that Eq.~\eqref{eq: CV optimization} takes negative values whenever no candidate split yields superior generalization to the current leaf node model. Based on this remark, we implement a greedy scheme to control the tree complexity, where branches that do not increase the score are pruned in a single bottom-up pass as post-processing, similarly to \cite{quinlan1987simplifying}. We further use a simple heuristic to drastically reduce training time, growing trees \textit{in stages} up to any desired maximum depth, successively pruning score-decreasing branches and regrowing remaining, non-pruned ones.
