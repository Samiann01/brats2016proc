%!TEX root = paper.tex

\section{Relation to Other Work}
\label{sec: related}

To the authors' knowledge, cross-validation estimates of the generalization error were not previously proposed as a node-level cost to optimize. Cross-validation and out-of-bag (OOB) estimates have been an integral component of bagging classifiers, for the purpose of \textit{monitoring} error and other key statistics, dating back to \eg Wolpert and Macready \cite{wolpert1999efficient} and Breiman \cite{breiman2001random}. In addition they have a long history for the purpose of optimal pruning of decision trees, as in the early work of Breiman et al.~\cite{breiman1984classification} (see \eg \cite{esposito1997comparative} for a review and analysis of such methods). Various sophisticated techniques such as minimum description length \cite{Quinlan:1989,MehtaEtAl:95} and other information-theoretic methods \cite{ForsythCW94} have also been employed solely for the purpose of pruning fully-grown trees. Our proposed approach differs in that the primary goal is to select better weak learners at each node, preventing trees from overfitting regardless of the technicalities controlling tree growth. The pruning strategy that ensues is closely related to that of Quinlan \cite{quinlan1987simplifying} and comes free of additional computations.  

Foundational research on random forest models remains active \cite{scornet2015consistency,scornet2016asymptotics,biau2016random} to close the persistent gap between simplified, idealized models and actual algorithms. Biau \cite{biau2012analysis} points out how crucially the behaviours of \textit{single} tree models and \textit{infinite} DF models differ in terms of generalization w.r.t. tree depth. With deep, fully grown trees, model averaging is one of the key mechanisms that induces consistent estimates. In practice data and computational resources are limited. Finite DF models are trained within the budget limits, so that the need for a principled control of model capacity node- and tree-wise is evident. Shotton et al.~\cite{shotton2013decision} give empirical cues as to the impact of tree depth and width on generalization. Note that \textit{decision jungles}' node merging capabilities fully stem from the fixed-width of the directed acyclic graph. The node splitting criterion introduced in the present work may constitute a worthy alternative to induce that mechanism without explicitely constraining width. 

The technical hurdles in experimenting with DFs and tuning them, in real-world applications, may explain in part why relatively little work has been done with regard to auto-context segmentation forests. Montillo et al.~\cite{montillo2011entangled} investigate an alternative where semantic context is built within a single DF layer, by designing entanglement features that can access ancestral nodes' label predictions at neighbouring voxels (cf. also \cite{kontschieder2012context}). The drawback is that a given tree can only access its own auto-context, which may be significantly weaker than the whole forest's. In the medical imaging literature, related work includes that of Zografos et al.~\cite{zografos2015hierarchical}, who intertwine a hierarchical supervoxel representation with two cascaded layers of gradient-boosted forests; and that of Gauriau et al.~\cite{gauriau2015multi} with a two-layer global-local DF-based framework for multi-organ localization. In contrast we cascade forests at will, by capitalizing on the ease of training each \textit{lifted} DF in the cascade.

%While the combination of auto-context and ROI refinement significantly benefits classification accuracy, we have also found the performance to be somewhat sensitive to the process by which the ROI is defined. Downstream auto-context layers tend to smooth boundary delineations, unless trained and ran on tighter ROIs (that emphasize the object, its boundary and direct surroundings). This results in a trade-off between better precision (tighter ROIs) and better recall (coarser ROIs). The point is seldom discussed in the literature and 

The motivation for clustering decision pathways is similar to that for the \textit{guided bagging} scheme proposed for Laplacian forests~\cite{lombaert2014laplacian} with the main difference that Lombaert et al.~cluster \textit{whole} images based on a predefined metric, whereas the proposed approach clusters image \textit{voxels} based on a metric induced by supervised classification forests. For instance in the multi-organ segmentation setting, the main clusters in our approach relate to individual organs over which downstream cluster-specific DFs effectively specialize. It is also related in spirit to the spatially-localized random forests of Zhang et al.~\cite{zhang2016concatenated}. Lastly, DFs have also been proposed specifically for tasks of clustering~\cite{konukoglu2013neighbourhood,conjeti2016metric}. Here, rather than introducing a separate clustering step, latent semantics captured by the classification DF's pathways are directly reused to guide classification in subsequent layers.